{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af6760da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, BertConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15bf2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "cosine_config = yaml.load(open('ALBEF/configs/Cosine-Retrieval.yaml', 'r'), Loader=yaml.Loader)\n",
    "itm_config = yaml.load(open('ALBEF/configs/ITM.yaml', 'r'), Loader=yaml.Loader)\n",
    "bert_config = json.load(open('ALBEF/configs/config_bert.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb35c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineConfig(PretrainedConfig): \n",
    "    model_type = 'XREM-Cosine'\n",
    "    default_bert_config = {'architectures': ['BertForMaskedLM'],\n",
    "                             'attention_probs_dropout_prob': 0.1,\n",
    "                             'hidden_act': 'gelu',\n",
    "                             'hidden_dropout_prob': 0.1,\n",
    "                             'hidden_size': 768,\n",
    "                             'initializer_range': 0.02,\n",
    "                             'intermediate_size': 3072,\n",
    "                             'layer_norm_eps': 1e-12,\n",
    "                             'max_position_embeddings': 512,\n",
    "                             'model_type': 'bert',\n",
    "                             'num_attention_heads': 12,\n",
    "                             'num_hidden_layers': 12,\n",
    "                             'pad_token_id': 0,\n",
    "                             'type_vocab_size': 2,\n",
    "                             'vocab_size': 30522,\n",
    "                             'fusion_layer': 6,\n",
    "                             'encoder_width': 768} \n",
    "    default_cosine_config = {\n",
    "                             'image_res': 256,\n",
    "                             'batch_size_train': 32,\n",
    "                             'batch_size_test': 64,\n",
    "                             'queue_size': 65536,\n",
    "                             'momentum': 0.995,\n",
    "                             'vision_width': 768,\n",
    "                             'embed_dim': 256,\n",
    "                             'temp': 0.07,\n",
    "                             'k_test': 128,\n",
    "                             'alpha': 0.4,\n",
    "                             'distill': True,\n",
    "                             'warm_up': True,\n",
    "                             'optimizer': {'opt': 'adamW', 'lr': '1e-5', 'weight_decay': 0.02},\n",
    "                             'schedular': {'sched': 'cosine',\n",
    "                              'lr': '1e-5',\n",
    "                              'epochs': 10,\n",
    "                              'min_lr': '1e-6',\n",
    "                              'decay_rate': 1,\n",
    "                              'warmup_lr': '1e-5',\n",
    "                              'warmup_epochs': 1,\n",
    "                              'cooldown_epochs': 0}}\n",
    "    def __init__(\n",
    "        self, \n",
    "        bert_config=default_bert_config, \n",
    "        cosine_config=default_cosine_config,\n",
    "        **kwargs\n",
    "    ): \n",
    "        self.bert_config=bert_config\n",
    "        super().__init__(**kwargs, **cosine_config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c91d28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITMConfig(PretrainedConfig): \n",
    "    model_type = 'XREM-ITM'\n",
    "    default_bert_config = {'architectures': ['BertForMaskedLM'],\n",
    "                             'attention_probs_dropout_prob': 0.1,\n",
    "                             'hidden_act': 'gelu',\n",
    "                             'hidden_dropout_prob': 0.1,\n",
    "                             'hidden_size': 768,\n",
    "                             'initializer_range': 0.02,\n",
    "                             'intermediate_size': 3072,\n",
    "                             'layer_norm_eps': 1e-12,\n",
    "                             'max_position_embeddings': 512,\n",
    "                             'model_type': 'bert',\n",
    "                             'num_attention_heads': 12,\n",
    "                             'num_hidden_layers': 12,\n",
    "                             'pad_token_id': 0,\n",
    "                             'type_vocab_size': 2,\n",
    "                             'vocab_size': 30522,\n",
    "                             'fusion_layer': 6,\n",
    "                             'encoder_width': 768} \n",
    "    default_itm_config = {\n",
    "                             'image_res': 384,\n",
    "                             'batch_size_train': 32,\n",
    "                             'batch_size_test': 64,\n",
    "                             'alpha': 0.4,\n",
    "                             'distill': True,\n",
    "                             'warm_up': False,\n",
    "                             'optimizer': {'opt': 'adamW', 'lr': '2e-5', 'weight_decay': 0.02},\n",
    "                             'schedular': {'sched': 'cosine',\n",
    "                              'lr': '2e-5',\n",
    "                              'epochs': 5,\n",
    "                              'min_lr': '1e-6',\n",
    "                              'decay_rate': 1,\n",
    "                              'warmup_lr': '1e-5',\n",
    "                              'warmup_epochs': 1,\n",
    "                              'cooldown_epochs': 0}}\n",
    "    def __init__(\n",
    "        self, \n",
    "        bert_config=default_bert_config, \n",
    "        itm_config=default_itm_config,\n",
    "        **kwargs\n",
    "    ): \n",
    "        self.bert_config=bert_config\n",
    "        super().__init__(**kwargs, **itm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e901780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "#Adapted cxr-repair\n",
    "#input: .h5 file containing the images\n",
    "class CXRTestDataset_h5(data.Dataset):\n",
    "    def __init__(self, img_path, input_resolution):\n",
    "        super().__init__()\n",
    "        self.img_dset = h5py.File(img_path, 'r')['cxr']\n",
    "        self.transform = transforms.Compose([\n",
    "                                            transforms.Resize((input_resolution,input_resolution),interpolation=Image.BICUBIC),\n",
    "                                            Normalize((101.48761, 101.48761, 101.48761), (83.43944, 83.43944, 83.43944))\n",
    "                                        ])\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_dset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = self.img_dset[idx]\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = np.repeat(img, 3, axis=0)\n",
    "        img = torch.from_numpy(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "#Adapted cxr-repair\n",
    "#input: files containing paths to the image files\n",
    "class CXRTestDataset(data.Dataset):\n",
    "    def __init__(self, target_files, input_resolution):\n",
    "        super().__init__()\n",
    "        self.files = target_files\n",
    "        self.transform = transforms.Compose([\n",
    "                                            transforms.Resize((input_resolution,input_resolution),interpolation=Image.BICUBIC),\n",
    "                                            Normalize((101.48761, 101.48761, 101.48761), (83.43944, 83.43944, 83.43944))\n",
    "                                        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        fpath = self.files[idx]\n",
    "        desired_size = 320\n",
    "        img = Image.open(fpath)\n",
    "        old_size = img.size\n",
    "        ratio = float(desired_size)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        img = img.resize(new_size, Image.ANTIALIAS)\n",
    "        new_img = Image.new('L', (desired_size, desired_size))\n",
    "        new_img.paste(img, ((desired_size-new_size[0])//2,\n",
    "                            (desired_size-new_size[1])//2))\n",
    "        img = np.asarray(new_img, np.float64)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = np.repeat(img, 3, axis=0)\n",
    "        img = torch.from_numpy(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "\n",
    "import models\n",
    "from models.model_itm import ALBEF as ALBEF_itm\n",
    "from models.model_retrieval import ALBEF as ALBEF_retrieval\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "from torchvision.transforms import Compose, Normalize, Resize, InterpolationMode\n",
    "import utils\n",
    "from PIL import Image\n",
    "from XREM_dataset import CXRTestDataset, CXRTestDataset_h5\n",
    "\n",
    "\n",
    "\n",
    "class RETRIEVAL_MODULE:\n",
    "\n",
    "    def __init__(self, \n",
    "                mode, \n",
    "                config, \n",
    "                checkpoint, \n",
    "                topk, \n",
    "                input_resolution, \n",
    "                delimiter, \n",
    "                max_token_len):\n",
    "                \n",
    "        self.mode = mode\n",
    "        assert mode == 'cosine-sim' or mode == 'image-text-matching', 'mode should be cosine-sim or image-text-matching'\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "        self.config = yaml.load(open(config, 'r'), Loader=yaml.Loader)\n",
    "        self.input_resolution = input_resolution\n",
    "        self.topk = topk\n",
    "        self.max_token_len = max_token_len\n",
    "        #self.dset = CXRTestDataset_h5(transform=self.transform, img_path=img_path)  \n",
    "        self.delimiter = delimiter\n",
    "        self.itm_labels = {'negative':0,  'positive':2}\n",
    "\n",
    "        if mode == 'cosine-sim':\n",
    "            self.load_albef_retrieval(checkpoint)\n",
    "        else:\n",
    "            self.load_albef_itm(checkpoint)\n",
    "\n",
    "\n",
    "    #adapted albef codebase\n",
    "    #For Image-Text Matching, we use ALBEF fine-tuned on visual entailmet to perform binary classification (entail/nonentail) \n",
    "    def load_albef_itm(self,checkpoint_path):\n",
    "        model = ALBEF_itm(config=self.config, \n",
    "                         text_encoder='bert-base-uncased', \n",
    "                         tokenizer=self.tokenizer\n",
    "                         ).to(self.device)  \n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu') \n",
    "        state_dict = checkpoint['model']\n",
    "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "        msg = model.load_state_dict(state_dict,strict=False)\n",
    "        model = model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    #adapted albef codebase\n",
    "    def load_albef_retrieval(self, checkpoint_path):\n",
    "        model = ALBEF_retrieval(config=self.config, \n",
    "                                text_encoder='bert-base-uncased', \n",
    "                                tokenizer=self.tokenizer\n",
    "                                ).to(device = self.device)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu') \n",
    "        state_dict = checkpoint['model']\n",
    "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "        m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)   \n",
    "        state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped \n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'bert' in key:\n",
    "                encoder_key = key.replace('bert.','')         \n",
    "                state_dict[encoder_key] = state_dict[key] \n",
    "                del state_dict[key]                \n",
    "        msg = model.load_state_dict(state_dict,strict=False)  \n",
    "        model = model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    def __forward__(self, images_dataset, reports):\n",
    "        if self.mode == 'cosine-sim':\n",
    "            embeddings = self.generate_embeddings(reports)\n",
    "            return self.cosine_sim_predict(images_dataset, reports, embeddings)\n",
    "        else: \n",
    "            return self.itm_predict(images_dataset, reports) \n",
    "\n",
    "    #adapted cxr-repair codebase\n",
    "    def generate_embeddings(self, reports, batch_size=2000):\n",
    "        #adapted albef codebase\n",
    "        def _embed_text(report):\n",
    "            with torch.no_grad():\n",
    "                text_input = self.tokenizer(report, \n",
    "                                            padding='max_length', \n",
    "                                            truncation=True, \n",
    "                                            max_length=self.max_token_len, \n",
    "                                            return_tensors=\"pt\").to(self.device) \n",
    "                text_output = self.model.text_encoder(text_input.input_ids, \n",
    "                                                        attention_mask = text_input.attention_mask, \n",
    "                                                        mode='text')  \n",
    "                text_feat = text_output.last_hidden_state\n",
    "                text_embed = F.normalize(self.model.text_proj(text_feat[:,0,:]))\n",
    "                text_embed /= text_embed.norm(dim=-1, keepdim=True)\n",
    "            return text_embed\n",
    "        num_batches = reports.shape[0] // batch_size\n",
    "        tensors = []\n",
    "        for i in tqdm(range(num_batches + 1)):\n",
    "            batch = list(reports[batch_size*i:min(batch_size*(i+1), len(self.reports))])\n",
    "            weights = _embed_text(batch)\n",
    "            tensors.append(weights)\n",
    "        embeddings = torch.cat(tensors)\n",
    "        return embeddings\n",
    "\n",
    "    #adapted cxr-repair codebase\n",
    "    def select_reports(self, reports, y_pred):      \n",
    "        reports_list = []\n",
    "        for i, simscores in tqdm(enumerate(y_pred)):\n",
    "            idxes = np.argsort(np.array(simscores))[-1 * self.topk:]\n",
    "            idxes = np.flip(idxes)\n",
    "            report = \"\"\n",
    "            for j in idxes: \n",
    "                if self.mode == 'cosine-sim':\n",
    "                    cand = reports[j]\n",
    "                else:\n",
    "                    cand = reports[i][j]\n",
    "                report += cand + self.delimiter\n",
    "            reports_list.append(report)\n",
    "        return reports_list\n",
    "\n",
    "    #adapted albef codebase\n",
    "    def itm_predict(self, images_dataset, reports):\n",
    "        y_preds = []\n",
    "        bs = 100\n",
    "        for i in tqdm(range(len(images_dataset))):\n",
    "            image = images_dataset[i].to(self.device, dtype = torch.float)\n",
    "            image = torch.unsqueeze(image, axis = 0)\n",
    "            image_embeds = self.model.visual_encoder(image)\n",
    "            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "            preds = torch.Tensor([]).to(self.device)\n",
    "            local_reports = reports[i]\n",
    "            for idx in range(0, len(local_reports), bs):\n",
    "                try:\n",
    "                    text = self.tokenizer(local_reports[idx:idx + bs], \n",
    "                                          padding='longest', \n",
    "                                          return_tensors=\"pt\").to(self.device) \n",
    "                    output = self.model.text_encoder(text.input_ids, \n",
    "                                attention_mask = text.attention_mask, \n",
    "                                encoder_hidden_states = image_embeds,\n",
    "                                encoder_attention_mask = image_atts,        \n",
    "                                return_dict = True\n",
    "                                )    \n",
    "                    prediction = self.model.cls_head(output.last_hidden_state[:,0,:])\n",
    "                    positive_score = prediction[:, self.itm_labels['positive']]\n",
    "                except:\n",
    "                    positive_score = torch.Tensor([0]).cuda()\n",
    "\n",
    "                preds = torch.cat([preds, positive_score])\n",
    "            idxes = torch.squeeze(preds).detach().cpu().numpy()\n",
    "            y_preds.append(idxes)\n",
    "            \n",
    "        df = pd.DataFrame(self.select_reports(reports, y_preds))\n",
    "        df.columns = [ \"Report Impression\"]\n",
    "        return df\n",
    "\n",
    "    #adapted cxr-repair codebase\n",
    "    def cosine_sim_predict(self, images_dataset, reports, embeddings): \n",
    "        def softmax(x):\n",
    "            return np.exp(x)/sum(np.exp(x))\n",
    "        def embed_img(images):\n",
    "            images = images.to(self.device, dtype = torch.float)\n",
    "            image_features = self.model.visual_encoder(images)        \n",
    "            image_features = self.model.vision_proj(image_features[:,0,:])            \n",
    "            image_features = F.normalize(image_features,dim=-1) \n",
    "            return image_features\n",
    "\n",
    "        y_pred = []\n",
    "        image_loader = torch.utils.data.DataLoader(images_dataset, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for image in tqdm(image_loader):\n",
    "                image_features = embed_img(image)\n",
    "                logits = image_features @ embeddings.T\n",
    "                logits = np.squeeze(logits.to('cpu').numpy(), axis=0).astype('float64')\n",
    "                norm_logits = (logits - logits.mean()) / (logits.std())\n",
    "                probs = softmax(norm_logits)\n",
    "                y_pred.append(probs)\n",
    "                \n",
    "        y_pred = np.array(np.array(y_pred))\n",
    "        df = pd.DataFrame(self.select_reports(reports, y_pred))\n",
    "        df.columns = [\"Report Impression\"]\n",
    "        return df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569dc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "class XREM (PreTrainedModel):\n",
    "#     config_class = XREMConfig\n",
    "    def __init__(self, config): \n",
    "        super().__init__(config)\n",
    "        \n",
    "#         df = pd.read_csv(args.impressions_path)\n",
    "#         impressions = df[\"report\"].drop_duplicates().dropna().reset_index(drop = True)\n",
    "        self.cosine_sim_module = RETRIEVAL_MODULE(mode='cosine-sim', \n",
    "                                             config=config.albef_retrieval_config, \n",
    "                                             checkpoint=config.albef_retrieval_ckpt, \n",
    "                                             topk=config.albef_retrieval_top_k,\n",
    "                                             input_resolution=256, \n",
    "                                             delimiter=config.albef_retrieval_delimiter, \n",
    "                                             max_token_len = 25)\n",
    "        \n",
    "\n",
    "        self.itm_module = RETRIEVAL_MODULE(impressions=new_impressions, \n",
    "                                                mode='image-text-matching', \n",
    "                                                config=config.albef_itm_config, \n",
    "                                                checkpoint=config.albef_itm_ckpt,\n",
    "                                                topk=config.albef_itm_top_k,\n",
    "                                                input_resolution=384, \n",
    "                                                img_path=config.img_path, \n",
    "                                                delimiter=config.albef_itm_delimiter, \n",
    "                                                max_token_len=30)\n",
    "        \n",
    "        \n",
    "    def __forward__(self, image_dataset, reports): \n",
    "        output = cosine_sim_module(images_dataset, reports)\n",
    "        if self.albef_itm_top_k > 0: \n",
    "            reports = [report.split(self.albef_retrieval_delimiter) for report in output['Report Impression']]\n",
    "            output = itm_module.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6152c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
