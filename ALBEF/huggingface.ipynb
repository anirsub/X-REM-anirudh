{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6760da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig, BertConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15bf2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "cosine_config = yaml.load(open('ALBEF/configs/Cosine-Retrieval.yaml', 'r'), Loader=yaml.Loader)\n",
    "itm_config = yaml.load(open('ALBEF/configs/ITM.yaml', 'r'), Loader=yaml.Loader)\n",
    "bert_config = json.load(open('ALBEF/configs/config_bert.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb35c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineConfig(PretrainedConfig): \n",
    "    model_type = 'XREM-Cosine'\n",
    "    default_bert_config = {'architectures': ['BertForMaskedLM'],\n",
    "                             'attention_probs_dropout_prob': 0.1,\n",
    "                             'hidden_act': 'gelu',\n",
    "                             'hidden_dropout_prob': 0.1,\n",
    "                             'hidden_size': 768,\n",
    "                             'initializer_range': 0.02,\n",
    "                             'intermediate_size': 3072,\n",
    "                             'layer_norm_eps': 1e-12,\n",
    "                             'max_position_embeddings': 512,\n",
    "                             'model_type': 'bert',\n",
    "                             'num_attention_heads': 12,\n",
    "                             'num_hidden_layers': 12,\n",
    "                             'pad_token_id': 0,\n",
    "                             'type_vocab_size': 2,\n",
    "                             'vocab_size': 30522,\n",
    "                             'fusion_layer': 6,\n",
    "                             'encoder_width': 768} \n",
    "    default_cosine_config = {\n",
    "                             'image_res': 256,\n",
    "                             'batch_size_train': 32,\n",
    "                             'batch_size_test': 64,\n",
    "                             'queue_size': 65536,\n",
    "                             'momentum': 0.995,\n",
    "                             'vision_width': 768,\n",
    "                             'embed_dim': 256,\n",
    "                             'temp': 0.07,\n",
    "                             'k_test': 128,\n",
    "                             'alpha': 0.4,\n",
    "                             'distill': True,\n",
    "                             'warm_up': True,\n",
    "                             'optimizer': {'opt': 'adamW', 'lr': '1e-5', 'weight_decay': 0.02},\n",
    "                             'schedular': {'sched': 'cosine',\n",
    "                              'lr': '1e-5',\n",
    "                              'epochs': 10,\n",
    "                              'min_lr': '1e-6',\n",
    "                              'decay_rate': 1,\n",
    "                              'warmup_lr': '1e-5',\n",
    "                              'warmup_epochs': 1,\n",
    "                              'cooldown_epochs': 0}}\n",
    "    def __init__(\n",
    "        self, \n",
    "        bert_config=default_bert_config, \n",
    "        cosine_config=default_cosine_config,\n",
    "        **kwargs\n",
    "    ): \n",
    "        self.bert_config=bert_config\n",
    "        super().__init__(**kwargs, **cosine_config)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c91d28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ITMConfig(PretrainedConfig): \n",
    "    model_type = 'XREM-ITM'\n",
    "    default_bert_config = {'architectures': ['BertForMaskedLM'],\n",
    "                             'attention_probs_dropout_prob': 0.1,\n",
    "                             'hidden_act': 'gelu',\n",
    "                             'hidden_dropout_prob': 0.1,\n",
    "                             'hidden_size': 768,\n",
    "                             'initializer_range': 0.02,\n",
    "                             'intermediate_size': 3072,\n",
    "                             'layer_norm_eps': 1e-12,\n",
    "                             'max_position_embeddings': 512,\n",
    "                             'model_type': 'bert',\n",
    "                             'num_attention_heads': 12,\n",
    "                             'num_hidden_layers': 12,\n",
    "                             'pad_token_id': 0,\n",
    "                             'type_vocab_size': 2,\n",
    "                             'vocab_size': 30522,\n",
    "                             'fusion_layer': 6,\n",
    "                             'encoder_width': 768} \n",
    "    default_itm_config = {\n",
    "                             'image_res': 384,\n",
    "                             'batch_size_train': 32,\n",
    "                             'batch_size_test': 64,\n",
    "                             'alpha': 0.4,\n",
    "                             'distill': True,\n",
    "                             'warm_up': False,\n",
    "                             'optimizer': {'opt': 'adamW', 'lr': '2e-5', 'weight_decay': 0.02},\n",
    "                             'schedular': {'sched': 'cosine',\n",
    "                              'lr': '2e-5',\n",
    "                              'epochs': 5,\n",
    "                              'min_lr': '1e-6',\n",
    "                              'decay_rate': 1,\n",
    "                              'warmup_lr': '1e-5',\n",
    "                              'warmup_epochs': 1,\n",
    "                              'cooldown_epochs': 0}}\n",
    "    def __init__(\n",
    "        self, \n",
    "        bert_config=default_bert_config, \n",
    "        itm_config=default_itm_config,\n",
    "        **kwargs\n",
    "    ): \n",
    "        self.bert_config=bert_config\n",
    "        super().__init__(**kwargs, **itm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e901780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "#Adapted cxr-repair\n",
    "#input: .h5 file containing the images\n",
    "class CXRTestDataset_h5(data.Dataset):\n",
    "    def __init__(self, img_path, input_resolution):\n",
    "        super().__init__()\n",
    "        self.img_dset = h5py.File(img_path, 'r')['cxr']\n",
    "        self.transform = transforms.Compose([\n",
    "                                            transforms.Resize((input_resolution,input_resolution),interpolation=Image.BICUBIC),\n",
    "                                            Normalize((101.48761, 101.48761, 101.48761), (83.43944, 83.43944, 83.43944))\n",
    "                                        ])\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_dset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img = self.img_dset[idx]\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = np.repeat(img, 3, axis=0)\n",
    "        img = torch.from_numpy(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img\n",
    "\n",
    "#Adapted cxr-repair\n",
    "#input: files containing paths to the image files\n",
    "class CXRTestDataset(data.Dataset):\n",
    "    def __init__(self, target_files, input_resolution):\n",
    "        super().__init__()\n",
    "        self.files = target_files\n",
    "        self.transform = transforms.Compose([\n",
    "                                            transforms.Resize((input_resolution,input_resolution),interpolation=Image.BICUBIC),\n",
    "                                            Normalize((101.48761, 101.48761, 101.48761), (83.43944, 83.43944, 83.43944))\n",
    "                                        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        fpath = self.files[idx]\n",
    "        desired_size = 320\n",
    "        img = Image.open(fpath)\n",
    "        old_size = img.size\n",
    "        ratio = float(desired_size)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        img = img.resize(new_size, Image.ANTIALIAS)\n",
    "        new_img = Image.new('L', (desired_size, desired_size))\n",
    "        new_img.paste(img, ((desired_size-new_size[0])//2,\n",
    "                            (desired_size-new_size[1])//2))\n",
    "        img = np.asarray(new_img, np.float64)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = np.repeat(img, 3, axis=0)\n",
    "        img = torch.from_numpy(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebcee837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "\n",
    "import models\n",
    "from models.model_itm import ALBEF as ALBEF_itm\n",
    "from models.model_retrieval import ALBEF as ALBEF_retrieval\n",
    "from models.vit import interpolate_pos_embed\n",
    "from models.tokenization_bert import BertTokenizer\n",
    "from torchvision.transforms import Compose, Normalize, Resize, InterpolationMode\n",
    "import utils\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class RETRIEVAL_MODULE:\n",
    "\n",
    "    def __init__(self, \n",
    "                mode, \n",
    "                config, \n",
    "                checkpoint, \n",
    "                topk, \n",
    "                input_resolution, \n",
    "                delimiter, \n",
    "                max_token_len):\n",
    "                \n",
    "        self.mode = mode\n",
    "        assert mode == 'cosine-sim' or mode == 'image-text-matching', 'mode should be cosine-sim or image-text-matching'\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "        self.config = yaml.load(open(config, 'r'), Loader=yaml.Loader)\n",
    "        self.input_resolution = input_resolution\n",
    "        self.topk = topk\n",
    "        self.max_token_len = max_token_len\n",
    "        #self.dset = CXRTestDataset_h5(transform=self.transform, img_path=img_path)  \n",
    "        self.delimiter = delimiter\n",
    "        self.itm_labels = {'negative':0,  'positive':2}\n",
    "\n",
    "        if mode == 'cosine-sim':\n",
    "            self.load_albef_retrieval(checkpoint)\n",
    "        else:\n",
    "            self.load_albef_itm(checkpoint)\n",
    "\n",
    "\n",
    "    #adapted albef codebase\n",
    "    #For Image-Text Matching, we use ALBEF fine-tuned on visual entailmet to perform binary classification (entail/nonentail) \n",
    "    def load_albef_itm(self,checkpoint_path):\n",
    "        model = ALBEF_itm(config=self.config, \n",
    "                         text_encoder='bert-base-uncased', \n",
    "                         tokenizer=self.tokenizer\n",
    "                         ).to(self.device)  \n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu') \n",
    "        state_dict = checkpoint['model']\n",
    "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "        msg = model.load_state_dict(state_dict,strict=False)\n",
    "        model = model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    #adapted albef codebase\n",
    "    def load_albef_retrieval(self, checkpoint_path):\n",
    "        model = ALBEF_retrieval(config=self.config, \n",
    "                                text_encoder='bert-base-uncased', \n",
    "                                tokenizer=self.tokenizer\n",
    "                                ).to(device = self.device)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu') \n",
    "        state_dict = checkpoint['model']\n",
    "        pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder)         \n",
    "        state_dict['visual_encoder.pos_embed'] = pos_embed_reshaped\n",
    "        m_pos_embed_reshaped = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],model.visual_encoder_m)   \n",
    "        state_dict['visual_encoder_m.pos_embed'] = m_pos_embed_reshaped \n",
    "        for key in list(state_dict.keys()):\n",
    "            if 'bert' in key:\n",
    "                encoder_key = key.replace('bert.','')         \n",
    "                state_dict[encoder_key] = state_dict[key] \n",
    "                del state_dict[key]                \n",
    "        msg = model.load_state_dict(state_dict,strict=False)  \n",
    "        model = model.eval()\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, images_dataset, reports):\n",
    "        if self.mode == 'cosine-sim':\n",
    "            embeddings = self.generate_embeddings(reports)\n",
    "            return self.cosine_sim_predict(images_dataset, reports, embeddings)\n",
    "        else: \n",
    "            return self.itm_predict(images_dataset, reports) \n",
    "\n",
    "    #adapted cxr-repair codebase\n",
    "    def generate_embeddings(self, reports, batch_size=2000):\n",
    "        #adapted albef codebase\n",
    "        def _embed_text(report):\n",
    "            with torch.no_grad():\n",
    "                text_input = self.tokenizer(report, \n",
    "                                            padding='max_length', \n",
    "                                            truncation=True, \n",
    "                                            max_length=self.max_token_len, \n",
    "                                            return_tensors=\"pt\").to(self.device) \n",
    "                text_output = self.model.text_encoder(text_input.input_ids, \n",
    "                                                        attention_mask = text_input.attention_mask, \n",
    "                                                        mode='text')  \n",
    "                text_feat = text_output.last_hidden_state\n",
    "                text_embed = F.normalize(self.model.text_proj(text_feat[:,0,:]))\n",
    "                text_embed /= text_embed.norm(dim=-1, keepdim=True)\n",
    "            return text_embed\n",
    "        num_batches = reports.shape[0] // batch_size\n",
    "        tensors = []\n",
    "        for i in tqdm(range(num_batches + 1)):\n",
    "            batch = list(reports[batch_size*i:min(batch_size*(i+1), len(reports))])\n",
    "            weights = _embed_text(batch)\n",
    "            tensors.append(weights)\n",
    "        embeddings = torch.cat(tensors)\n",
    "        return embeddings\n",
    "\n",
    "    #adapted cxr-repair codebase\n",
    "    def select_reports(self, reports, y_pred):      \n",
    "        reports_list = []\n",
    "        for i, simscores in tqdm(enumerate(y_pred)):\n",
    "            idxes = np.argsort(np.array(simscores))[-1 * self.topk:]\n",
    "            idxes = np.flip(idxes)\n",
    "            report = \"\"\n",
    "            for j in idxes: \n",
    "                if self.mode == 'cosine-sim':\n",
    "                    cand = reports[j]\n",
    "                else:\n",
    "                    cand = reports[i][j]\n",
    "                report += cand + self.delimiter\n",
    "            reports_list.append(report)\n",
    "        return reports_list\n",
    "\n",
    "    #adapted albef codebase\n",
    "    def itm_predict(self, images_dataset, reports):\n",
    "        y_preds = []\n",
    "        bs = 100\n",
    "        for i in tqdm(range(len(images_dataset))):\n",
    "            image = images_dataset[i].to(self.device, dtype = torch.float)\n",
    "            image = torch.unsqueeze(image, axis = 0)\n",
    "            image_embeds = self.model.visual_encoder(image)\n",
    "            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "            preds = torch.Tensor([]).to(self.device)\n",
    "            local_reports = reports[i]\n",
    "            for idx in range(0, len(local_reports), bs):\n",
    "                try:\n",
    "                    text = self.tokenizer(local_reports[idx:idx + bs], \n",
    "                                          padding='longest', \n",
    "                                          return_tensors=\"pt\").to(self.device) \n",
    "                    output = self.model.text_encoder(text.input_ids, \n",
    "                                attention_mask = text.attention_mask, \n",
    "                                encoder_hidden_states = image_embeds,\n",
    "                                encoder_attention_mask = image_atts,        \n",
    "                                return_dict = True\n",
    "                                )    \n",
    "                    prediction = self.model.cls_head(output.last_hidden_state[:,0,:])\n",
    "                    positive_score = prediction[:, self.itm_labels['positive']]\n",
    "                except:\n",
    "                    positive_score = torch.Tensor([0]).cuda()\n",
    "\n",
    "                preds = torch.cat([preds, positive_score])\n",
    "            idxes = torch.squeeze(preds).detach().cpu().numpy()\n",
    "            y_preds.append(idxes)\n",
    "            \n",
    "        df = pd.DataFrame(self.select_reports(reports, y_preds))\n",
    "        df.columns = [ \"Report Impression\"]\n",
    "        return df\n",
    "\n",
    "    #adapted cxr-repair codebase\n",
    "    def cosine_sim_predict(self, images_dataset, reports, embeddings): \n",
    "        def softmax(x):\n",
    "            return np.exp(x)/sum(np.exp(x))\n",
    "        def embed_img(images):\n",
    "            images = images.to(self.device, dtype = torch.float)\n",
    "            image_features = self.model.visual_encoder(images)        \n",
    "            image_features = self.model.vision_proj(image_features[:,0,:])            \n",
    "            image_features = F.normalize(image_features,dim=-1) \n",
    "            return image_features\n",
    "\n",
    "        y_pred = []\n",
    "        image_loader = torch.utils.data.DataLoader(images_dataset, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for image in tqdm(image_loader):\n",
    "                image_features = embed_img(image)\n",
    "                logits = image_features @ embeddings.T\n",
    "                logits = np.squeeze(logits.to('cpu').numpy(), axis=0).astype('float64')\n",
    "                norm_logits = (logits - logits.mean()) / (logits.std())\n",
    "                probs = softmax(norm_logits)\n",
    "                y_pred.append(probs)\n",
    "                \n",
    "        y_pred = np.array(np.array(y_pred))\n",
    "        df = pd.DataFrame(self.select_reports(reports, y_pred))\n",
    "        df.columns = [\"Report Impression\"]\n",
    "        return df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae127523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "569dc16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XREMConfig {\n",
       "  \"albef_itm_ckpt\": \"../../jaehwan_CXR_ReFusE/ALBEF/output/sample/ve/checkpoint_7.pth\",\n",
       "  \"albef_itm_config\": \"configs/ITM.yaml\",\n",
       "  \"albef_itm_delimiter\": \"[SEP]\",\n",
       "  \"albef_itm_top_k\": 10,\n",
       "  \"albef_retrieval_ckpt\": \"../../jaehwan_CXR_ReFusE/ALBEF/output/sample/pretrain/checkpoint_59.pth\",\n",
       "  \"albef_retrieval_config\": \"configs/Cosine-Retrieval.yaml\",\n",
       "  \"albef_retrieval_delimiter\": \"[SEP]\",\n",
       "  \"albef_retrieval_top_k\": 50,\n",
       "  \"model_type\": \"XREM\",\n",
       "  \"transformers_version\": \"4.8.1\"\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XREMConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65797269",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrem = XREM(XREMConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04af91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-1655415bfd1c>:15: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize((input_resolution,input_resolution),interpolation=Image.BICUBIC),\n",
      "/home/jj229/anaconda3/envs/albef/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dset_cosine =  CXRTestDataset_h5('../../jaehwan_CXR_ReFusE/data/cxr.h5', 256)\n",
    "dset_itm =  CXRTestDataset_h5('../../jaehwan_CXR_ReFusE/data/cxr.h5', 384)\n",
    "df = pd.read_csv('../../jaehwan_CXR_ReFusE/data/mimic_train_impressions.csv')\n",
    "reports = df[\"report\"].drop_duplicates().dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ecccf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:35<00:00,  1.43s/it]\n",
      "100%|██████████| 3678/3678 [01:13<00:00, 49.77it/s]\n",
      "3678it [00:42, 86.28it/s]\n",
      "100%|██████████| 3678/3678 [16:25<00:00,  3.73it/s]\n",
      "3678it [00:00, 35237.80it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39b85f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP chest compared to ___:  Lungs are still fully expanded and clear, cardiomediastinal and hilar silhouettes and pleural surfaces are normal.[SEP]Clear lungs with no evidence of residual pneumonia.[SEP]AP chest compared to ___:  Lungs fully expanded and clear.  Normal cardiomediastinal and hilar silhouettes and pleural surfaces.[SEP]AP chest compared to ___:  Lungs clear, normal cardiomediastinal and hilar silhouettes and pleural surfaces.[SEP]AP chest compared to ___:  Lungs are well expanded and clear.  Normal cardiomediastinal, hilar silhouettes and pleural surfaces.[SEP]AP chest compared to ___:  There is no longer pulmonary edema.  Cardiomediastinal and hilar silhouettes and pleural surfaces are normal and the lungs are clear.[SEP]NG tube in the stomach.  Resolution of prior bibasilar pulmonary opacities.[SEP]AP chest reviewed in the absence of prior chest imaging:  Lungs are fully expanded and essentially clear.  Normal cardiomediastinal and hilar silhouettes and pleural surfaces.  Right upper quadrant drainage catheter noted, but cannot be localized.[SEP]AP chest compared to ___:  Lungs are fully expanded and clear.  Normal cardiomediastinal and hilar silhouettes and pleural surfaces.  An ascending dual-channel catheter ends in the right atrium.[SEP]AP chest compared to ___:  Lungs are essentially clear.  A region of previous peribronchial infiltration in the left mid lung has resolved.  There is no pulmonary edema or pleural effusion and the heart size is top normal.[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(output.iloc[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd2821dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XREM()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41c04b64",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XREM' object has no attribute '_attr_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-20f3c9fbde8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxrem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attr_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/albef/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XREM' object has no attribute '_attr_'"
     ]
    }
   ],
   "source": [
    "xrem._attr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ba1222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrem.config.albef_retrieval_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835c58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from XREM import XREM\n",
    "from XREM_config import XREMConfig\n",
    "from XREM_dataset import CXRTestDataset_h5\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0296a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrem = XREM(XREMConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43be051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj229/anaconda3/envs/albef/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dset_cosine =  CXRTestDataset_h5('../../jaehwan_CXR_ReFusE/data/cxr.h5', 256)\n",
    "dset_itm =  CXRTestDataset_h5('../../jaehwan_CXR_ReFusE/data/cxr.h5', 384)\n",
    "df = pd.read_csv('../../jaehwan_CXR_ReFusE/data/mimic_train_impressions.csv')\n",
    "reports = df[\"report\"].drop_duplicates().dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300558a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:33<00:00,  1.39s/it]\n",
      "100%|██████████| 3678/3678 [01:14<00:00, 49.57it/s]\n",
      "3678it [00:46, 79.88it/s]\n",
      "  6%|▌         | 203/3678 [00:56<16:00,  3.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-da5b0ab5e58c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxrem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset_cosine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdset_itm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/albef/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/data1/hms/dbmi/rajpurkar/lab/home/jj229/refuse_github/ALBEF/XREM.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, reports, image_dataset_cosine, image_dataset_itm)\u001b[0m\n",
      "\u001b[0;32m/n/data1/hms/dbmi/rajpurkar/lab/home/jj229/refuse_github/ALBEF/XREM_module.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, images_dataset, reports)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_sim_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitm_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m#adapted cxr-repair codebase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/data1/hms/dbmi/rajpurkar/lab/home/jj229/refuse_github/ALBEF/XREM_module.py\u001b[0m in \u001b[0;36mitm_predict\u001b[0;34m(self, images_dataset, reports)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0midxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0my_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = xrem(reports, dset_cosine, dset_itm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0ef03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
