# CXR_ReFusE

## Setup

CXR_ReFusE makes use of multiple github repos. Make sure to include all of them under CXR_ReFusE directory. 

* [ifcc](https://github.com/ysmiura/ifcc)
    * download model_medrad_19k.tar.gz by running `resources/download.sh`
    * Merge `ifcc_CXR_ReFusE_code` with the original directory
* [CheXbert](https://github.com/stanfordmlgroup/CheXbert)
    * download the [pretrained weights](https://stanfordmedicine.box.com/s/c3stck6w6dol3h36grdc97xoydzxd7w9) as well 
* [CXR-RePaiR](https://github.com/rajpurkarlab/CXR-RePaiR)
* [CXR-Report-Metric](https://github.com/rajpurkarlab/CXR-Report-Metric)
   * Merge `CXR-Report-Metric_CXR_ReFusE_code` with the original directory

As we made multiple edits to the ALBEF directory, please refer to the ALBEF directory uploaded here instead of cloning a new one. 

Download the [zipfile](https://drive.google.com/file/d/1VW8q0b4Jh6Pj3crpFHTRC3mTCRUjI2zi/view?usp=sharing) containing our dataset and place them in the appropriate folders. 


## Data-preprocessing
Here we use CXR-RePaiR's data preprocessing steps.
Refer to "Data Preprocessing" in cxr-repair for more detail. 
We obtain `data/cxr.h5`, `data/mimic_train_impressions.csv`, `data/mimic_test_impressions.csv` from CXR-RePaiR

## Training

For pretraining & fine-tuning ALBEF, use `pretrain_script.sh` and ve_script.sh in `ALBEF` directory. 
For pretraining, use `data/mimic_train.json`
For finetuning, use `data/ve_train.json` that is generated by `data/generate_ve_train_file.py`. 
`Jaehwan_edits.txt` keeps track of the edits I made to the original ALBEF github codebase. 

## Inference
`inference.sh` first calls `ALBEF/CXR_ReFusE_pipeline.py` to use cosine-sim & ve scores to select k' reports. 
`inference.sh` then calls `ifcc/m2trans_nli_filter.py` to select k reports based on nli scores
"Report Impression" column contains the reports before applying the nli filter, 
"filtered" column contains the filtered reports. 
    
## Evaluation
For evaluating the generated reports, use CXR-Report-Metric. 
Note that the original paper computed the metric scores using a subset of the mimic test set. 
First activate the conda env for CXR-Report-Metric
Next use `prepare_df.py` to select the inferences for the corresponding 2,192 samples from our generation. 
Then use `test_metric.py` to generate the scores. 
Finally use `compute_avg_score.py` to find the average scores. 

## Supplementary Experiments
Here we introduce several additional experiments we conducted with CXR_ReFusE approach. 

### VE vs No VE
We observe that appending the visual entailment step at the end significantly boosts the model performance. To disable the visual entailment step, comment out the second call to the retrieval module in `CXR_ReFusE_pipeline.py`, adjust the topk value for the first module to 2, and set the delimiter value to `' '`. 

### Report filtering vs no report filtering
We also observe that appending the filtering step improves the model performance. To skip the redundancy filtering, comment out `m2trans_nli_filter.py` in `inference.sh` and adjust the topk value for the VE output to 2. 

### Bertscore vs NLI score for measuing 
One potential limitation of using nli filter is the fact that it is not symmetric. If the candidate report captures a broader context than our current concatenation of reports, the nli filter will return "neutral" despite the overlapping content. To address this issue, we tried using Bertscore as a measure of sentence similarity and used maximum f1 threshold of 0.5 to filter out redundant reports. To replicate this experiment, replace `m2trans_nli_filter.py` with `bertscore_nli_filter.py` in `inference.sh`


