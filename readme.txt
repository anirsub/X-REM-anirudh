For data-preprocessing, use cxr-repair's data preprocessing steps. 
    Refer to "Data Preprocessing" in https://github.com/rajpurkarlab/CXR-RePaiR for more detail. 
    We obtain data/cxr.h5, data/mimic_train_impressions.csv, data/mimic_test_impressions.csv

For pretraining & fine-tuning ALBEF, use pretrain_script.sh and ve_script.sh in ALBEF directory. 
    For pretraining, use data/mimic_train.json
    For finetuning, use data/ve_train.json (generated by generate_ve_train_file.py)
    Jaehwan_edits.txt keeps track of the edits I made to the original ALBEF github codebase. 

For making inference, use inference.sh in this directory. 
    inference.sh first calls ALBEF/python3 CXR-ReFusE-pipeline.py to use cosine-sim & ve scores to select k' reports. 
    inference.sh then calls ifcc/m2trans_nli_filter.py to select k reports based on nli scores
    "Report Impression" column contains the reports before applying the nli filter, 
    "filtered" column contains the filtered reports. 
    
For evaluating the generated reports, use cxr-report-metric (https://github.com/rajpurkarlab/CXR-Report-Metric)
    Note that the original paper computed the metric scores using a subset of the mimic test set. 
    First activate the conda env for CXR-Report-Metric
    Next use prepare_df.py to select the inferences for the corresponding 2,192 samples from our generation. 
    Then use test_metric.py to generate the scores. 
    Finally use compute_avg_score.py to find the average scores. 